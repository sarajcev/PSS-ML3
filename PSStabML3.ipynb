{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold learning in Power System Transient Stability Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import Isomap, TSNE, MDS\n",
    "from sklearn.manifold import SpectralEmbedding as SE\n",
    "from sklearn.manifold import LocallyLinearEmbedding as LLE\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics.pairwise import paired_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using experimental HalvingRandomSearchCV for hyperparameters optimization.\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annealing import simulated_annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure aesthetics\n",
    "sns.set(context='paper', style='white', font_scale=1.1)\n",
    "sns.set_style('ticks', {'xtick.direction':'in', 'ytick.direction':'in'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power System Transient Stability Analysis Data (IEEE Benchmark Test Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('GridDictionary2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Percentage of \"ones\" in the \"Stability\" column.\n",
    "print('There is {:.1f}% of unstable cases in the dataset!'\n",
    "      .format(data['Stability'].sum()/float(len(data['Stability']))*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_features = len(data.columns) - 1\n",
    "X_data = data.iloc[:, 0:no_features]  # features\n",
    "print('X_data', X_data.shape)\n",
    "y_data = data['Stability']\n",
    "print('y_data', y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratify shuffle split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, train_size=0.8, stratify=y_data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Unstable cases in training dataset: {:.1f}%:'\n",
    "      .format(np.sum(y_train)/float(len(y_train))*100.))\n",
    "print('Unstable cases in testing dataset {:.1f}%:'\n",
    "      .format(np.sum(y_test)/float(len(y_test))*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable cases index values.\n",
    "idx_stable = y_test==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring models using cross-validated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_default(X, y):\n",
    "    \"\"\" Scoring default SVC model. \"\"\"\n",
    "    # Score with default hyperparameters.\n",
    "    scores = cross_val_score(svm.SVC(kernel='rbf', class_weight='balanced'), \n",
    "                             X, y, cv=3, scoring='f1')\n",
    "    print('Score using 3-fold CV: {:g} +/- {:g}'\n",
    "          .format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_optimized(X, y, C, gamma):\n",
    "    \"\"\" Scoring optimized SVC model. \"\"\"\n",
    "    # Score with the optimized hyperparameters.\n",
    "    scores = cross_val_score(svm.SVC(C=C, gamma=gamma, kernel='rbf', \n",
    "                                     class_weight='balanced'), \n",
    "                             X, y, cv=3, scoring='f1', n_jobs=-1)\n",
    "    print('Score using 3-fold CV: {:g} +/- {:g}'\n",
    "          .format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projection(X, idx):\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    ax.scatter(X[idx,0], X[idx,1], \n",
    "            s=20, c='green', marker='o', edgecolors='k', alpha=0.5, label='Stable')\n",
    "    ax.scatter(X[~idx,0], X[~idx,1], \n",
    "            s=20, c='red', marker='o', edgecolors='k', alpha=0.5, label='Unstable')\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlabel('First component')\n",
    "    ax.set_ylabel('Second component')\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the input data.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following dimmensionality reduction methods are examined:\n",
    "\n",
    "* **PCA** (principal components analysis)\n",
    "* **kPCA** (kernelized principal components analysis)\n",
    "* **tSVD** (truncated singular value decomposition)\n",
    "* **iMAP** (isomap embedding)\n",
    "* **t-SNE** (T-distributed stochastic neighbor embedding)\n",
    "* **LLE** (locally linear embedding)\n",
    "* **LLE:LTSA** (locally linear embedding with local tangent space alignment algorithm)\n",
    "* **LLE:H** (locally linear embedding with Hessian eigenmap method)\n",
    "* **SE** (spectral embedding)\n",
    "* **MDS** (multi-dimensional scaling)\n",
    "\n",
    "Some of these have their own hyperparameters (e.g. KernelPCA) which can be optimized together with the hyperparameters of the SVC estimator. This will be shown for the KernelPCA method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization with simulated annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated annealing is used for optimizing hyperparameters of the SVC estimator only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_cv(C, gamma, X_data, y_data):\n",
    "    \"\"\" \n",
    "    SVC cross validation.\n",
    "    \n",
    "    This function will instantiate a SVC classifier with a \n",
    "    RBF kernel and hyper-parameters C and gamma. Combined\n",
    "    with data and targets it will be used to perform cross\n",
    "    validation. The goal is to find combinations of C and\n",
    "    gamma that maximizes the `f1` scoring metric.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    C: float\n",
    "        Regularization parameter (penalty is a squared l2). \n",
    "    gamma: float\n",
    "        Kernel coefficient.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cval: float\n",
    "        Mean value of the score from the cross-validation.\n",
    "    \"\"\"\n",
    "    # Instantiate SVC with RBF kernel and class weight balancing.\n",
    "    estimator = svm.SVC(C=C, gamma=gamma, kernel='rbf', \n",
    "                        class_weight='balanced', probability=True)\n",
    "    # Score the estimator using cross validation.\n",
    "    cval = cross_val_score(estimator, X_data, y_data, \n",
    "                           scoring='f1', cv=2, n_jobs=-1)\n",
    "    \n",
    "    return -cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_svc(X_data, y_data, x0, bounds=None, coolC=10., sigma=1., \n",
    "                 burn=10, eps=1e-6, verbose=False):\n",
    "    \"\"\" Simulated Annealing to optimize SVC hyperparameters. \"\"\"\n",
    "    \n",
    "    def svc_crossval(expC, expGamma):\n",
    "        \"\"\" \n",
    "        Wrapper for the SVC cross-validation function.\n",
    "        \"\"\"\n",
    "        # Exploring parameters in 'log' space.\n",
    "        C = 10**expC\n",
    "        gamma = 10**expGamma\n",
    "        model_instance = svc_cv(C, gamma, X_data, y_data)\n",
    "        \n",
    "        return model_instance\n",
    "\n",
    "    # Simulated Annealing.\n",
    "    x, E = simulated_annealing(svc_crossval, x0, bounds=bounds,\n",
    "                               C=coolC, sigma=sigma, burn=burn, eps=eps,\n",
    "                               verbose=verbose)\n",
    "    \n",
    "    return x, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature schedule.\n",
    "x = np.arange(start=1, stop=200, step=1)\n",
    "T0 = 1.\n",
    "y1 = T0*np.exp(-x/10)\n",
    "y2 = T0*0.9**(x)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6.5,2.5))\n",
    "ax[0].plot(x, y1, lw=2, label='T0*exp(-k/10)')\n",
    "ax[0].plot(x, y2, lw=2, label='T0*0.9**k')\n",
    "ax[0].set_xlabel('Iterations')\n",
    "ax[0].set_ylabel('Temperature')\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[0].grid()\n",
    "ax[1].semilogy(x, y1, lw=2, label='T0*exp(-k/10)')\n",
    "ax[1].semilogy(x, y2, lw=2, label='T0*0.9**k')\n",
    "ax[1].set_xlabel('Iterations')\n",
    "ax[1].set_ylabel('Temperature')\n",
    "ax[1].legend(loc='lower left')\n",
    "ax[1].grid(which='both')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values (C, gamma) for SVC optimization.\n",
    "x0 = np.array([1., -2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many components are needed for the 90% explained variance?\n",
    "pca = PCA(n_components=0.9, svd_solver='full').fit(X_train)\n",
    "X_pca = pca.transform(X_test)\n",
    "print(X_pca.shape)\n",
    "# Score with the 90% explained variance.\n",
    "score_default(X_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction.\n",
    "# Set `whiten` to True/False to see if there is any difference.\n",
    "pca = PCA(n_components=2, whiten=True).fit(X_train)\n",
    "X_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_pca, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_pca, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_pca, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction using KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters of the KernelPCA are optimized by means of the **unsupervised** learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpca_metric(X_data, gamma=None):\n",
    "    kpca = KernelPCA(n_components=2, kernel='rbf', gamma=gamma,\n",
    "                     fit_inverse_transform=True, n_jobs=-1)\n",
    "    X_embedded = kpca.fit_transform(X_data)\n",
    "    X_reconstructed = kpca.inverse_transform(X_embedded)\n",
    "    # Compute paired distances between embedding and its reconstruction.\n",
    "    distances = paired_distances(X_data, X_reconstructed, metric='euclidean')\n",
    "\n",
    "    return distances.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_kpca(X_data, x0, coolC=10., sigma=1., \n",
    "                      burn=10, eps=1e-6):\n",
    "    \"\"\" Simulated Annealing for kPCA hyperparameters. \"\"\"\n",
    "\n",
    "    def kpca(expGamma):\n",
    "        # Exploring parameters in 'log' space.\n",
    "        gamma_kpca = 10**expGamma\n",
    "        model_instance = kpca_metric(X_data, gamma=gamma_kpca)\n",
    "        \n",
    "        return model_instance\n",
    "\n",
    "    # Simulated Annealing.\n",
    "    x, E = simulated_annealing(kpca, x0, C=coolC, sigma=sigma, \n",
    "                               burn=burn, eps=eps)\n",
    "    \n",
    "    return x, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_metric(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial value for the Gamma-kPCA\n",
    "xk0 = np.array([-2.])\n",
    "# Optimize kPCA hyperparameters using simulated annealing.\n",
    "x, E = optimize_kpca(X_train, xk0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_opt = KernelPCA(n_components=2, kernel='rbf', \n",
    "                     gamma=10**x[0], # optimal kernel value\n",
    "                     n_jobs=-1).fit(X_train)\n",
    "X_kpca_opt = kpca_opt.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_kpca_opt, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_kpca_opt, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_kpca_opt, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_kpca_opt, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated annealing is here used for optimizing the hyperparameters of the KernelPCA and the SVC estimator **at the same time**, using the **supervised** learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpca_svc_cv(C, gamma, gamma_kpca, X_data, y_data):\n",
    "    \"\"\" SVC cross validation with KernelPCA. \"\"\"\n",
    "    # Instantiate SVC with RBF kernel and class weight balancing.\n",
    "    estimator = svm.SVC(C=C, gamma=gamma, kernel='rbf', \n",
    "                        class_weight='balanced', probability=True)\n",
    "    reduction = KernelPCA(n_components=2, kernel='rbf', gamma=gamma_kpca)\n",
    "    pipe = Pipeline([\n",
    "        ('kpca', reduction),\n",
    "        ('svm', estimator)\n",
    "    ])\n",
    "    # Score the estimator using cross validation.\n",
    "    cval = cross_val_score(pipe, X_data, y_data, \n",
    "                           scoring='f1', cv=2, n_jobs=-1)\n",
    "    \n",
    "    return -cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_kpca_svc(X_data, y_data, x0, \n",
    "                      coolC=10., sigma=1., burn=10, eps=1e-6):\n",
    "    \"\"\" Simulated Annealing for SVC & kPCA hyperparameters. \"\"\"\n",
    "\n",
    "    def kpca_svc_crossval(expC, expGamma, expGammkPCA):\n",
    "        \"\"\" Wrapper for the cross-validation function. \"\"\"\n",
    "        # Exploring parameters in 'log' space.\n",
    "        C = 10**expC\n",
    "        gamma = 10**expGamma\n",
    "        gamma_kpca = 10**expGammkPCA\n",
    "        model_instance = kpca_svc_cv(C, gamma, gamma_kpca, X_data, y_data)\n",
    "        \n",
    "        return model_instance\n",
    "\n",
    "    # Simulated Annealing.\n",
    "    x, E = simulated_annealing(kpca_svc_crossval, x0, \n",
    "                               C=coolC, sigma=sigma, burn=burn, eps=eps)\n",
    "    \n",
    "    return x, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values (C-SVM, Gamma-SVM, Gamma-kPCA)\n",
    "xk0 = np.array([1., -2., -1.])\n",
    "# Optimize kPCA & SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_kpca_svc(X_train, y_train, xk0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_opt = KernelPCA(n_components=2, kernel='rbf', \n",
    "                     gamma=10**x[2], # optimal kernel value\n",
    "                     n_jobs=-1).fit(X_train)\n",
    "X_kpca_opt = kpca_opt.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_kpca_opt, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_kpca_opt, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_kpca_opt, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KernelPCA without the kPCA kernel optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce features in the dataset down to only 2 principal components.\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', n_jobs=-1).fit(X_train)\n",
    "X_kpca = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_kpca, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_kpca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bounds on SVC hyperparameters in log-space.\n",
    "# Parameter C: 0.001 to 10000.\n",
    "# Parameter gamma: 0.0001 to 10.\n",
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_kpca, y_test, \n",
    "                    x0, bounds=[(-3,4), (-4,1)], \n",
    "                    burn=20, eps=1e-14, verbose=True)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_kpca, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search CV for SVC hyperparameters optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search type: ['random', 'halving']\n",
    "search = 'random'\n",
    "\n",
    "reduction = KernelPCA(n_components=2, kernel='rbf')\n",
    "estimator = svm.SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "pipe = Pipeline([\n",
    "    ('kpca', reduction),\n",
    "    ('svm', estimator)\n",
    "])\n",
    "parameters = {\n",
    "    'kpca__gamma': stats.expon(scale=.1),\n",
    "    'svm__C':stats.expon(scale=100), \n",
    "    'svm__gamma':stats.expon(scale=.1)\n",
    "}\n",
    "if search == 'random':\n",
    "    model = RandomizedSearchCV(estimator=pipe,\n",
    "                               param_distributions=parameters,\n",
    "                               n_iter=200,\n",
    "                               cv=2, scoring='f1',\n",
    "                               refit=True, n_jobs=-1)\n",
    "elif search == 'halving':\n",
    "    model = HalvingRandomSearchCV(estimator=pipe, \n",
    "                                  param_distributions=parameters, \n",
    "                                  cv=2, scoring='f1',\n",
    "                                  refit=True, n_jobs=-1)\n",
    "else:\n",
    "    raise NotImplementedError(f'Search method: {search} not recognized.')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_test, y_test, cv=3, scoring='f1', n_jobs=-1)\n",
    "print('Average score using 3-fold CV: {:.4f} +/- {:.4f}'\n",
    "      .format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction using truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2).fit(X_train)\n",
    "X_svd = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_svd, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_svd, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_svd, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = Isomap(n_components=2, n_neighbors=100, n_jobs=-1).fit(X_train)\n",
    "X_iso = iso.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_iso, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_iso, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_iso, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_iso, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE with optimized hyperparameters\n",
    "\n",
    "Here hyperparameters of the t-SNE (i.e. `perplexity`) is optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_metric(perplex, X_data):\n",
    "    \"\"\" KL divergence of the TSNE as a metric. \"\"\"\n",
    "    reduction = TSNE(n_components=2, perplexity=perplex, n_jobs=-1)\n",
    "    # Score the estimator.\n",
    "    cval = reduction.fit(X_data)\n",
    "    \n",
    "    return cval.kl_divergence_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_tsne(X_data, x0, coolC=10., sigma=1., burn=10, eps=1e-6, \n",
    "                  verbose=False):\n",
    "    \"\"\" Simulated Annealing for t-SNE hyperparameters. \"\"\"\n",
    "\n",
    "    def tsne(expPerplex):\n",
    "        \"\"\" Wrapper for the cross-validation function. \"\"\"\n",
    "        from numpy import exp\n",
    "\n",
    "        # Exploring parameter perplexity in natural logarithm (ln) space.\n",
    "        perplex = exp(expPerplex)\n",
    "        instance = tsne_metric(perplex, X_data)\n",
    "        \n",
    "        return instance\n",
    "\n",
    "    # Simulated Annealing.\n",
    "    x, E = simulated_annealing(tsne, x0, \n",
    "                               C=coolC, sigma=sigma, burn=burn, \n",
    "                               eps=eps, verbose=verbose)\n",
    "    \n",
    "    return x, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values (perplexity)\n",
    "xt0 = np.array([3.])\n",
    "# Optimize t-SNE & SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_tsne(X_train, xt0, burn=20, eps=1e-10, verbose=True)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_opt = TSNE(n_components=2, \n",
    "                  perplexity=np.exp(x[0]), # optimal value\n",
    "                  n_jobs=-1).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_tsne_opt, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_tsne_opt, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_tsne_opt, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_tsne_opt, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE without hyperparameters optimization\n",
    "\n",
    "Hyperparameters of the t-SNE are not being optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = TSNE(n_components=2, n_jobs=-1).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_tsne, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_tsne, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_tsne, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_tsne, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Linear Embedding (LLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LLE(n_components=2, n_neighbors=10, \n",
    "          method='standard', n_jobs=-1).fit(X_train)\n",
    "X_lle = lle.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_lle, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_lle, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_lle, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_lle, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally linear embedding with LTSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltsa = LLE(n_components=2, n_neighbors=10, \n",
    "           method='ltsa', eigen_solver='dense', n_jobs=-1).fit(X_train)\n",
    "X_ltsa = ltsa.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_ltsa, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_ltsa, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_ltsa, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_ltsa, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally linear embedding with Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hess = LLE(n_components=2, n_neighbors=100, \n",
    "           method='hessian', n_jobs=-1).fit(X_train)\n",
    "X_hess = hess.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_hess, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_hess, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_hess, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_hess, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified locally linear embedding (MLLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlle = LLE(n_components=2, n_neighbors=50, \n",
    "           method='modified', n_jobs=-1).fit(X_train)\n",
    "X_mlle = lle.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_mlle, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_mlle, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_mlle, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_mlle, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spec = SE(n_components=2, affinity='nearest_neighbors', \n",
    "            n_jobs=-1).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_spec, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_spec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_spec, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_spec, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-dimensional scaling (MDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `metric` to True/False to see if there is any difference.\n",
    "X_mds = MDS(n_components=2, metric=True, n_jobs=-1).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_mds, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_default(X_mds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_mds, y_test, x0, burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_optimized(X_mds, y_test, C=10**x[0], gamma=10**x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use \"best\" SVC parameters here.\n",
    "C = 10**x[0]\n",
    "gamma = 10**x[1]\n",
    "best_parameters = {'C': C, 'gamma': gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = cross_val_predict(svm.SVC(**best_parameters, probability=True, \n",
    "                                     class_weight='balanced'), \n",
    "                             X_test, y_test, cv=3, \n",
    "                             method='predict_proba',\n",
    "                             n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = y_probas[:,1]  # score == probability of positive class\n",
    "precisions, recalls, thresholds = metrics.precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4.5,4.5))\n",
    "ax.plot(precisions, recalls, lw=2, label='SVC')\n",
    "default = np.argmin(np.abs(thresholds - 0.5))\n",
    "ax.plot(precisions[default], recalls[default], '^', c='k', markersize=10, \n",
    "        label='Threshold = 0.5', fillstyle='none', mew=2)\n",
    "ax.set_xlabel('Precision')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.legend(loc='best')\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot decision region for test samples with only two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select projected data.\n",
    "X_test_best = X_mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate SVC from selected projection.\n",
    "svc_best = svm.SVC(C=C, gamma=gamma, kernel='rbf', \n",
    "                   class_weight='balanced', \n",
    "                   probability=True).fit(X_test_best, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.1; delta = 0.01\n",
    "x_min, x_max = X_test_best[:,0].min() - h, X_test_best[:,0].max() + h\n",
    "y_min, y_max = X_test_best[:,1].min() - h, X_test_best[:,1].max() + h\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, delta), np.arange(y_min, y_max, delta))\n",
    "Z = svc_best.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "ax.scatter(X_test_best[idx_stable,0], X_test_best[idx_stable,1], \n",
    "           s=30, c='green', marker='o', edgecolors='k', alpha=0.5, label='Stable')\n",
    "ax.scatter(X_test_best[~idx_stable,0], X_test_best[~idx_stable,1], \n",
    "           s=30, c='red', marker='o', edgecolors='k', alpha=0.5, label='Unstable')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('1st component')\n",
    "ax.set_ylabel('2nd component')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, IPython, sklearn, scipy, matplotlib\n",
    "print(\"Notebook createad with:\\\n",
    "      \\nPython {:s}\\nIPython {:s}\\nScikit-learn {:s}\\nPandas {:s}\\nNumpy \\\n",
    "      {:s}\\nScipy {:s}\\nMatplotlib {:s}\"\\\n",
    "      .format(sys.version[:5], IPython.__version__, sklearn.__version__, \n",
    "              pd.__version__, np.__version__, scipy.__version__, \n",
    "              matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
