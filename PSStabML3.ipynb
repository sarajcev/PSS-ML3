{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold learning in Power System Transient Stability Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import Isomap, TSNE, MDS\n",
    "from sklearn.manifold import SpectralEmbedding as SE\n",
    "from sklearn.manifold import LocallyLinearEmbedding as LLE\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics.pairwise import paired_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using experimental HalvingRandomSearchCV for hyperparameters optimization.\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annealing import simulated_annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure aesthetics\n",
    "sns.set(context='paper', style='white', font_scale=1.1)\n",
    "sns.set_style('ticks', {'xtick.direction':'in', 'ytick.direction':'in'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power System Transient Stability Analysis Data (IEEE Benchmark Test Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('GridDictionary2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Percentage of \"ones\" in the \"Stability\" column.\n",
    "print('There is {:.1f}% of unstable cases in the dataset!'\n",
    "      .format(data['Stability'].sum()/float(len(data['Stability']))*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_features = len(data.columns) - 1\n",
    "X_data = data.iloc[:, 0:no_features]  # features\n",
    "print('X_data', X_data.shape)\n",
    "y_data = data['Stability']\n",
    "print('y_data', y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratify shuffle split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, train_size=0.8, stratify=y_data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Unstable cases in training dataset: {:.1f}%:'\n",
    "      .format(np.sum(y_train)/float(len(y_train))*100.))\n",
    "print('Unstable cases in testing dataset {:.1f}%:'\n",
    "      .format(np.sum(y_test)/float(len(y_test))*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable cases index values.\n",
    "idx_stable = y_test==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring models using cross-validated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_default(X, y):\n",
    "    \"\"\" Scoring default SVC model. \"\"\"\n",
    "    # Score with default hyperparameters.\n",
    "    scores = cross_val_score(svm.SVC(kernel='rbf', class_weight='balanced'), \n",
    "                             X, y, cv=3, scoring='f1')\n",
    "    print('Score using 3-fold CV: {:g} +/- {:g}'\n",
    "          .format(np.mean(scores), np.std(scores)))\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_optimized(X, y, C, gamma):\n",
    "    \"\"\" Scoring optimized SVC model. \"\"\"\n",
    "    # Score with the optimized hyperparameters.\n",
    "    scores = cross_val_score(svm.SVC(C=C, gamma=gamma, kernel='rbf', \n",
    "                                     class_weight='balanced'), \n",
    "                             X, y, cv=3, scoring='f1', n_jobs=-1)\n",
    "    print('Score using 3-fold CV: {:g} +/- {:g}'\n",
    "          .format(np.mean(scores), np.std(scores)))\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projection(X, idx):\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    ax.scatter(X[idx,0], X[idx,1], \n",
    "            s=20, c='green', marker='o', edgecolors='k', alpha=0.5, label='Stable')\n",
    "    ax.scatter(X[~idx,0], X[~idx,1], \n",
    "            s=20, c='red', marker='o', edgecolors='k', alpha=0.5, label='Unstable')\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlabel('First component')\n",
    "    ax.set_ylabel('Second component')\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the input data.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following dimmensionality reduction methods are examined:\n",
    "\n",
    "* **PCA** (principal components analysis)\n",
    "* **kPCA** (kernelized principal components analysis)\n",
    "* **tSVD** (truncated singular value decomposition)\n",
    "* **iMAP** (isomap embedding)\n",
    "* **t-SNE** (T-distributed stochastic neighbor embedding)\n",
    "* **LLE** (locally linear embedding)\n",
    "* **LLE:LTSA** (locally linear embedding with local tangent space alignment algorithm)\n",
    "* **LLE:H** (locally linear embedding with Hessian eigenmap method)\n",
    "* **SE** (spectral embedding)\n",
    "* **MDS** (multi-dimensional scaling)\n",
    "\n",
    "Some of these have their own hyperparameters (e.g. KernelPCA) which can be optimized together with the hyperparameters of the SVC estimator. This will be shown for the KernelPCA method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization with simulated annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated annealing is used for optimizing hyperparameters of the SVC estimator only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_cv(C, gamma, X_data, y_data):\n",
    "    \"\"\" \n",
    "    Support Vector Machine Classifier cross-validation.\n",
    "    \n",
    "    This function will instantiate a SVC classifier with a \n",
    "    RBF kernel and hyper-parameters \"C\" and \"gamma\". Combined\n",
    "    with data and targets it will be used to perform cross-\n",
    "    validation. The goal is to find combinations of \"C\" and\n",
    "    \"gamma\" that maximizes the `f1` scoring metric.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    C: float\n",
    "        Regularization parameter (penalty is a squared l2). \n",
    "    gamma: float\n",
    "        Kernel coefficient.\n",
    "    X_data: np.array\n",
    "        Matrix (2d array) of features.\n",
    "    y_data: np.array\n",
    "        Vector (1d array) of targets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cval: float\n",
    "        Mean value of the score from the cross-validation.\n",
    "    \"\"\"\n",
    "    # Instantiate SVC with RBF kernel and class weight balancing.\n",
    "    estimator = svm.SVC(C=C, gamma=gamma, kernel='rbf', \n",
    "                        class_weight='balanced', probability=True)\n",
    "    # Score the estimator using cross validation.\n",
    "    cval = cross_val_score(estimator, X_data, y_data, \n",
    "                           scoring='f1', cv=2, n_jobs=-1)\n",
    "    \n",
    "    return -cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_svc(X_data, y_data, x0, bounds=None, coolC=10., sigma=0.5, \n",
    "                 fs=0.1, burn=10, eps=1e-8, verbose=False):\n",
    "    \"\"\" \n",
    "    Simulated Annealing to optimize SVC hyperparameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_data: np.array\n",
    "        Matrix (2d array) of features.\n",
    "    y_data: np.array\n",
    "        Vector (1d array) of targets.\n",
    "    x0: np.array or list\n",
    "        Initial values for the variables of the energy function.\n",
    "        For example, if the energy functions is f(x,y), then\n",
    "        initial values are given as [x0, y0], where x0 is the \n",
    "        initial value for the variable x and y0 for the variable\n",
    "        y. Order of array elements is important!\n",
    "    bounds: list of tuples or None, default=None\n",
    "        List of two-element tuples which define bounds on energy \n",
    "        function variables. The number and order of list elements\n",
    "        is the same as for the array `x0` for the initial values.\n",
    "        For example, if the energy function is f(x,y), then \n",
    "        bounds are defined as follows: [(xl,xu), (yl,yu)], where\n",
    "        xl, xu are, respectively, lower and upper bounds for the\n",
    "        variable x, and yl, yu represnt the same limits for the\n",
    "        variable y. Order of tuples in the list is important!\n",
    "    coolC: float, default=10.\n",
    "        Constant decay value of the temperature scheduling.\n",
    "        Temperature after k-th iteration is determined from \n",
    "        the following relation: Tk = T0*exp(-k/C).\n",
    "    sigma: float, default=0.5\n",
    "        Standard deviation of a statistical distribution for\n",
    "        the random walk by which new candidates are generated. \n",
    "    fs: float, default=0.1\n",
    "        Factor for reducing the standard deviation of a statisti-\n",
    "        cal distribution used for the random walk (see parameter\n",
    "        `sigma` above for more information). Default value halves\n",
    "        the `sigma` after the burn-in.    \n",
    "    burn: int, default=10\n",
    "        Number of iterations with the original step size of\n",
    "        the random walk from the Student's t distribution, after\n",
    "        which the step size is reduced approximately by a factor\n",
    "        of 10 by switching over to the Normal distribution with\n",
    "        a lower standard deviation (see parameter `sigma` above).\n",
    "    eps: float, default=1e-8\n",
    "        Temperature value at which the algorithm is stopped.\n",
    "    verbose: bool, default=False\n",
    "        Indicator for printing (on stdout) internal messages.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: np.array\n",
    "        Optimal values of the SVC hyperparameters \"C\" and \"gamma\",\n",
    "        respectively, as first and second element of this 1d array.\n",
    "    E: float\n",
    "        SVC classifier's metric's optimal value.\n",
    "    \"\"\"\n",
    "    \n",
    "    def svc_crossval(expC, expGamma):\n",
    "        \"\"\" \n",
    "        Wrapper for the SVC cross-validation function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        expC: float\n",
    "            Log value of the SVC regularization parameter. \n",
    "        expGamma: float\n",
    "            Log value of the SVC's RBF kernel coefficient.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        cv_score: float\n",
    "            Negative value of the cross-validated score.\n",
    "        \"\"\"\n",
    "        # Exploring parameters in 'log' space.\n",
    "        C = 10**expC\n",
    "        gamma = 10**expGamma\n",
    "        cv_score = svc_cv(C, gamma, X_data, y_data)\n",
    "        \n",
    "        return cv_score\n",
    "\n",
    "    # Simulated Annealing.\n",
    "    x, E = simulated_annealing(svc_crossval, x0, bounds=bounds,\n",
    "                               C=coolC, sigma=sigma, fs=fs, \n",
    "                               burn=burn, eps=eps, verbose=verbose)\n",
    "    \n",
    "    return x, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature schedule.\n",
    "x = np.arange(start=1, stop=200, step=1)\n",
    "T0 = 1.\n",
    "y1 = T0*np.exp(-x/10)\n",
    "y2 = T0*0.9**(x)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6.5,2.5))\n",
    "ax[0].plot(x, y1, lw=2, label='T0*exp(-k/10)')\n",
    "ax[0].plot(x, y2, lw=2, label='T0*0.9**k')\n",
    "ax[0].set_xlabel('Iterations')\n",
    "ax[0].set_ylabel('Temperature')\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[0].grid()\n",
    "ax[1].semilogy(x, y1, lw=2, label='T0*exp(-k/10)')\n",
    "ax[1].semilogy(x, y2, lw=2, label='T0*0.9**k')\n",
    "ax[1].set_xlabel('Iterations')\n",
    "ax[1].set_ylabel('Temperature')\n",
    "ax[1].legend(loc='lower left')\n",
    "ax[1].grid(which='both')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values (C, gamma) for SVC optimization.\n",
    "x0 = np.array([1., -2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for holding model scores.\n",
    "scores_table = {}\n",
    "# Dictionary for holding SVM parameters.\n",
    "svm_table = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many components are needed for the 90% explained variance?\n",
    "pca = PCA(n_components=0.9, svd_solver='full').fit(X_train)\n",
    "X_pca = pca.transform(X_test)\n",
    "print(X_pca.shape)\n",
    "# Score with the 90% explained variance.\n",
    "mu, sigma = score_default(X_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction.\n",
    "# Set `whiten` to True/False to see if there is any difference.\n",
    "pca = PCA(n_components=2, whiten=True).fit(X_train)\n",
    "X_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_pca, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_pca, y_test)\n",
    "scores_table['PCA_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_pca, y_test, x0, bounds=[(-1,3), (-4,1)], \n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['PCA'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_pca, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['PCA_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction using KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters of the KernelPCA are optimized by means of the **unsupervised** learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpca_metric(X_data, gamma=None):\n",
    "    \"\"\"\n",
    "    Distance metric for the kPCA.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_data: np.array\n",
    "        Matrix (2d array) of features.\n",
    "    gamma: float\n",
    "        Kernel value for the kPCA RBF kernel.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    distance: float\n",
    "        Sum of (euclidean) distances between the original\n",
    "        features and their reconstruction after embedding\n",
    "        and its inversion.\n",
    "    \"\"\"\n",
    "    kpca = KernelPCA(n_components=2, kernel='rbf', gamma=gamma,\n",
    "                     fit_inverse_transform=True, n_jobs=-1)\n",
    "    X_embedded = kpca.fit_transform(X_data)\n",
    "    X_reconstructed = kpca.inverse_transform(X_embedded)\n",
    "    # Compute paired distances between embedding and its reconstruction.\n",
    "    distances = paired_distances(X_data, X_reconstructed, metric='euclidean')\n",
    "\n",
    "    return distances.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_kpca(X_data, x0, bounds=None, coolC=10., sigma=0.5,\n",
    "                  fs=0.1, burn=10, eps=1e-8, verbose=False):\n",
    "    \"\"\" Simulated Annealing for kPCA hyperparameters. \"\"\"\n",
    "\n",
    "    def kpca(expGamma):\n",
    "        # Exploring parameters in 'log' space.\n",
    "        gamma_kpca = 10**expGamma\n",
    "        distance = kpca_metric(X_data, gamma=gamma_kpca)\n",
    "        \n",
    "        return distance\n",
    "\n",
    "    # Simulated Annealing.\n",
    "    x, E = simulated_annealing(kpca, x0, bounds=bounds, C=coolC, sigma=sigma, \n",
    "                               fs=fs, burn=burn, eps=eps, verbose=verbose)\n",
    "    \n",
    "    return x, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_metric(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial value for the Gamma-kPCA\n",
    "xk0 = np.array([-2.])\n",
    "# Optimize kPCA hyperparameters using simulated annealing.\n",
    "x, E = optimize_kpca(X_train, xk0, bounds=[(-4,1)], burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_opt = KernelPCA(n_components=2, kernel='rbf', \n",
    "                     gamma=10**x[0], # optimal kernel value\n",
    "                     n_jobs=-1).fit(X_train)\n",
    "X_kpca_opt = kpca_opt.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_kpca_opt, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_kpca_opt, y_test)\n",
    "scores_table['kPCA_un_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_kpca_opt, y_test, x0, bounds=[(-1,3), (-4,1)], \n",
    "                    burn=20, eps=1e-10, verbose=True)\n",
    "print(x, E)\n",
    "svm_table['kPCA_un'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_kpca_opt, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['kPCA_un_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated annealing is here used for optimizing the hyperparameters of the KernelPCA and the SVC estimator **at the same time**, using the **supervised** learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpca_svc_cv(C, gamma, gamma_kpca, X_data, y_data):\n",
    "    \"\"\" \n",
    "    SVC cross-validation with KernelPCA. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    C: float\n",
    "        Regularization parameter of the SVC. \n",
    "    gamma: float\n",
    "        Kernel coefficient of the SVC RBF kernel.\n",
    "    gamma_kpca; float\n",
    "        Kernel coefficient of the kPCA RBF kernel.\n",
    "    X_data: np.array\n",
    "        Matrix (2d array) of features.\n",
    "    y_data: np.array\n",
    "        Vector (1d array) of targets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cval: float\n",
    "        Mean value of the score from the cross-validation.\n",
    "    \"\"\"\n",
    "    # Instantiate SVC with RBF kernel and class weight balancing.\n",
    "    estimator = svm.SVC(C=C, gamma=gamma, kernel='rbf', \n",
    "                        class_weight='balanced', probability=True)\n",
    "    # Instantiate kPCA with RBF kernel.\n",
    "    reduction = KernelPCA(n_components=2, kernel='rbf', gamma=gamma_kpca)\n",
    "    # Create a pipeline.\n",
    "    pipe = Pipeline([\n",
    "        ('kpca', reduction),\n",
    "        ('svm', estimator)\n",
    "    ])\n",
    "    # Score the pipeline using cross-validation.\n",
    "    cval = cross_val_score(pipe, X_data, y_data, \n",
    "                           scoring='f1', cv=2, n_jobs=-1)\n",
    "    \n",
    "    return -cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_kpca_svc(X_data, y_data, x0, bounds=None,\n",
    "                      coolC=10., sigma=0.5, fs=0.1, burn=10, \n",
    "                      eps=1e-8, verbose=False):\n",
    "    \"\"\" Simulated Annealing for SVC & kPCA hyperparameters. \"\"\"\n",
    "\n",
    "    def kpca_svc_crossval(expC, expGamma, expGammkPCA):\n",
    "        \"\"\" \n",
    "        Wrapper for the cross-validation function. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        expC: float\n",
    "            Log value of the SVC regularization parameter. \n",
    "        expGamma: float\n",
    "            Log value of the SVC's RBF kernel coefficient.\n",
    "        expGammkPCA: float\n",
    "            Log value of the kPCA's RBF kernel coefficient.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cv_score: float\n",
    "            Negative value of the cross-validated score.        \n",
    "        \"\"\"\n",
    "        # Exploring parameters in 'log' space.\n",
    "        C = 10**expC\n",
    "        gamma = 10**expGamma\n",
    "        gamma_kpca = 10**expGammkPCA\n",
    "        cv_score = kpca_svc_cv(C, gamma, gamma_kpca, X_data, y_data)\n",
    "        \n",
    "        return cv_score\n",
    "\n",
    "    # Simulated Annealing.\n",
    "    x, E = simulated_annealing(kpca_svc_crossval, x0, bounds=bounds,\n",
    "                               C=coolC, sigma=sigma, fs=fs, burn=burn, \n",
    "                               eps=eps, verbose=verbose)\n",
    "    \n",
    "    return x, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values (C-SVM, Gamma-SVM, Gamma-kPCA)\n",
    "xk0 = np.array([1., -2., -1.])\n",
    "# Optimize kPCA & SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_kpca_svc(X_train, y_train, \n",
    "                         xk0, bounds=[(-1,3), (-4,1), (-4,1)],\n",
    "                         burn=20, eps=1e-10)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_opt = KernelPCA(n_components=2, kernel='rbf', \n",
    "                     gamma=10**x[2], # optimal kernel value\n",
    "                     n_jobs=-1).fit(X_train)\n",
    "X_kpca_opt = kpca_opt.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_kpca_opt, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_kpca_opt, y_test)\n",
    "scores_table['kPCA_w_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_kpca_opt, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['kPCA_w_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KernelPCA without the kPCA kernel optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce features in the dataset down to only 2 principal components.\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', n_jobs=-1).fit(X_train)\n",
    "X_kpca = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_kpca, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_kpca, y_test)\n",
    "scores_table['kPCA_wo_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bounds on SVC hyperparameters in log-space.\n",
    "# Parameter C: 0.001 to 10000.\n",
    "# Parameter gamma: 0.0001 to 10.\n",
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_kpca, y_test, \n",
    "                    x0, bounds=[(-1,3), (-4,1)],\n",
    "                    burn=20, eps=1e-10, verbose=True)\n",
    "print(x, E)\n",
    "svm_table['kPCA_wo'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_kpca, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['kPCA_wo_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search CV for SVC hyperparameters optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search type: ['random', 'halving']\n",
    "search = 'random'\n",
    "\n",
    "reduction = KernelPCA(n_components=2, kernel='rbf')\n",
    "estimator = svm.SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "pipe = Pipeline([\n",
    "    ('kpca', reduction),\n",
    "    ('svm', estimator)\n",
    "])\n",
    "parameters = {\n",
    "    'kpca__gamma': stats.expon(scale=.1),\n",
    "    'svm__C':stats.expon(scale=100), \n",
    "    'svm__gamma':stats.expon(scale=.1)\n",
    "}\n",
    "if search == 'random':\n",
    "    model = RandomizedSearchCV(estimator=pipe,\n",
    "                               param_distributions=parameters,\n",
    "                               n_iter=200,\n",
    "                               cv=2, scoring='f1',\n",
    "                               refit=True, n_jobs=-1)\n",
    "elif search == 'halving':\n",
    "    # Experimental method.\n",
    "    model = HalvingRandomSearchCV(estimator=pipe, \n",
    "                                  param_distributions=parameters, \n",
    "                                  cv=2, scoring='f1',\n",
    "                                  refit=True, n_jobs=-1)\n",
    "else:\n",
    "    raise NotImplementedError(f'Search method: {search} not recognized.')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_test, y_test, cv=3, scoring='f1', n_jobs=-1)\n",
    "print('Average score using 3-fold CV: {:.4f} +/- {:.4f}'\n",
    "      .format(np.mean(scores), np.std(scores)))\n",
    "scores_table['kPCA_rand'] = [np.mean(scores), np.std(scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction using truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2).fit(X_train)\n",
    "X_svd = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_svd, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_svd, y_test)\n",
    "scores_table['SVD_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_svd, y_test, x0, bounds=[(-1,3), (-4,1)], \n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['SVD'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_svd, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['SVD_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = Isomap(n_components=2, n_neighbors=100, n_jobs=-1).fit(X_train)\n",
    "X_iso = iso.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_iso, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_iso, y_test)\n",
    "scores_table['ISO_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_iso, y_test, x0, bounds=[(-1,3), (-4,1)], \n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['ISO'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_iso, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['ISO_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE with optimized hyperparameters\n",
    "\n",
    "Here hyperparameters of the t-SNE (i.e. `perplexity`) is optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_metric(perplex, X_data):\n",
    "    \"\"\" t-SNE metric based on the KL divergence. \"\"\"\n",
    "    from numpy import log\n",
    "\n",
    "    reduction = TSNE(n_components=2, perplexity=perplex, n_jobs=-1)\n",
    "    # Score the estimator.\n",
    "    cval = reduction.fit(X_data)\n",
    "\n",
    "    kl = cval.kl_divergence_\n",
    "    n = float(X_data.shape[0])\n",
    "    parameters = reduction.get_params(deep=False)\n",
    "    perp = parameters['perplexity']\n",
    "\n",
    "    metric = 2*kl + log(n)*(perp/n)\n",
    "    \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_tsne(X_data, x0, bounds=None, coolC=10., sigma=0.5, \n",
    "                  fs=0.1, burn=10, eps=1e-6, verbose=False):\n",
    "    \"\"\" Simulated Annealing for t-SNE hyperparameters. \"\"\"\n",
    "\n",
    "    def tsne(expPerplex):\n",
    "        from numpy import exp\n",
    "\n",
    "        # Exploring parameter perplexity in natural logarithm (ln) space.\n",
    "        perplex = exp(expPerplex)\n",
    "        # KL divergence based score.\n",
    "        kl_metric = tsne_metric(perplex, X_data)\n",
    "        \n",
    "        return kl_metric\n",
    "\n",
    "    # Simulated Annealing.\n",
    "    x, E = simulated_annealing(tsne, x0, bounds=bounds, C=coolC, \n",
    "                               sigma=sigma, fs=fs, burn=burn, \n",
    "                               eps=eps, verbose=verbose)\n",
    "    \n",
    "    return x, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values (perplexity)\n",
    "xt0 = np.array([3.])\n",
    "# Optimize t-SNE & SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_tsne(X_train, xt0, bounds=[(0.,4.)], sigma=0.2,\n",
    "                     burn=20, eps=1e-8, verbose=True)\n",
    "print(x, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_opt = TSNE(n_components=2, \n",
    "                  perplexity=np.exp(x[0]), # optimal value\n",
    "                  n_jobs=-1).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_tsne_opt, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_tsne_opt, y_test)\n",
    "scores_table['SNE_w_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_tsne_opt, y_test, x0, bounds=[(-1,3), (-4,1)],\n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['SNE_w'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_tsne_opt, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['SNE_w_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE without hyperparameters optimization\n",
    "\n",
    "Hyperparameters of the t-SNE are not being optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = TSNE(n_components=2, n_jobs=-1).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_tsne, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_tsne, y_test)\n",
    "scores_table['SNE_wo_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_tsne, y_test, x0, bounds=[(-1,3), (-4,1)], \n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['SNE_wo'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_tsne, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['SNE_wo_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Linear Embedding (LLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LLE(n_components=2, n_neighbors=10, \n",
    "          method='standard', n_jobs=-1).fit(X_train)\n",
    "X_lle = lle.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_lle, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_lle, y_test)\n",
    "scores_table['LLE_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_lle, y_test, x0, bounds=[(-1,3), (-4,1)],\n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['LLE'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_lle, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['LLE_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally linear embedding with LTSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltsa = LLE(n_components=2, n_neighbors=10, \n",
    "           method='ltsa', eigen_solver='dense', n_jobs=-1).fit(X_train)\n",
    "X_ltsa = ltsa.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_ltsa, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_ltsa, y_test)\n",
    "scores_table['TSA_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_ltsa, y_test, x0, bounds=[(-1,3), (-4,1)],\n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['TSA'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_ltsa, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['TSA_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally linear embedding with Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hess = LLE(n_components=2, n_neighbors=100, \n",
    "           method='hessian', n_jobs=-1).fit(X_train)\n",
    "X_hess = hess.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_hess, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_hess, y_test)\n",
    "scores_table['HES_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_hess, y_test, x0, bounds=[(-1,3), (-4,1)],\n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['HES'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_hess, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['HES_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified locally linear embedding (MLLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlle = LLE(n_components=2, n_neighbors=50, \n",
    "           method='modified', n_jobs=-1).fit(X_train)\n",
    "X_mlle = lle.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_mlle, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_mlle, y_test)\n",
    "scores_table['MLE_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_mlle, y_test, x0, bounds=[(-1,3), (-4,1)],\n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['MLE'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_mlle, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['MLE_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spec = SE(n_components=2, affinity='nearest_neighbors', \n",
    "            n_jobs=-1).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_spec, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_spec, y_test)\n",
    "scores_table['SPE_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_spec, y_test, x0, bounds=[(-1,3), (-4,1)],\n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['SPE'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_spec, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['SPE_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-dimensional scaling (MDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `metric` to True/False to see if there is any difference.\n",
    "X_mds = MDS(n_components=2, metric=True, n_jobs=-1).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(X_mds, idx_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_default(X_mds, y_test)\n",
    "scores_table['MDS_def'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize SVC hyperparameters using simulated annealing.\n",
    "x, E = optimize_svc(X_mds, y_test, x0, bounds=[(-1,3), (-4,1)],\n",
    "                    burn=20, eps=1e-10)\n",
    "print(x, E)\n",
    "svm_table['MDS'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = score_optimized(X_mds, y_test, C=10**x[0], gamma=10**x[1])\n",
    "scores_table['MDS_opt'] = [mu, sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate scores from all models into a dataframe.\n",
    "pdscores = pd.DataFrame(data=scores_table)\n",
    "pdscores = pdscores.transpose()\n",
    "pdscores.columns = ['Mean', 'Std']\n",
    "pdscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Interesting models for additional testing and comparisons: 'kPCA', 'SVD', 'ISO', 'MDS' and 't-SNE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate hyperparameters from different SVMs into a dataframe.\n",
    "svms = pd.DataFrame(data=svm_table)\n",
    "svms = svms.transpose()\n",
    "svms.columns = ['C', 'Gamma']\n",
    "# Convert back from the log-scale.\n",
    "svms = svms.apply(lambda x: 10**x)\n",
    "svms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Tradeoff for a selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select SVC model here from the following list:\n",
    "# ['PCA', 'kPCA_un', 'kPCA_wo', 'SVD', 'ISO', 'SNE_w', 'SNE_wo', 'LLE',\n",
    "#  'TSA', 'HES', 'MLE', 'SPE', 'MDS']\n",
    "model = 'kPCA_wo'\n",
    "C = svms['C'].loc[model]\n",
    "gamma = svms['Gamma'].loc[model]\n",
    "best_parameters = {'C': C, 'gamma': gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = cross_val_predict(svm.SVC(**best_parameters, probability=True, \n",
    "                                     class_weight='balanced'), \n",
    "                             X_test, y_test, cv=3, \n",
    "                             method='predict_proba',\n",
    "                             n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = y_probas[:,1]  # score == probability of positive class\n",
    "precisions, recalls, thresholds = metrics.precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.plot(precisions, recalls, lw=2, label='SVC')\n",
    "default = np.argmin(np.abs(thresholds - 0.5))\n",
    "ax.plot(precisions[default], recalls[default], '^', c='k', markersize=8, \n",
    "        label='Threshold = 0.5', fillstyle='none', mew=2)\n",
    "ax.set_xlabel('Precision')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.legend(loc='best')\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot decision region for the selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projected data (X_test) that is consistent \n",
    "# with the previously selected model.\n",
    "if model == 'PCA':\n",
    "    X_test_mod = X_pca\n",
    "elif model == 'kPCA_un':\n",
    "    X_test_mod = X_kpca_opt\n",
    "elif model == 'kPCA_wo':\n",
    "    X_test_mod = X_kpca\n",
    "elif model == 'SVD':\n",
    "    X_test_mod = X_svd\n",
    "elif model == 'ISO':\n",
    "    X_test_mod = X_iso\n",
    "elif model == 'SNE_w':\n",
    "    X_test_mod = X_tsne_opt\n",
    "elif model == 'SNE_wo':\n",
    "    X_test_mod = X_tsne\n",
    "elif model == 'LLE':\n",
    "    X_test_mod = X_lle\n",
    "elif model == 'TSA':\n",
    "    X_test_mod = X_ltsa\n",
    "elif model == 'HES':\n",
    "    X_test_mod = X_hess\n",
    "elif model == 'MLE':\n",
    "    X_test_mod = X_mlle\n",
    "elif model == 'SPE':\n",
    "    X_test_mod = X_spec\n",
    "elif model == 'MDS':\n",
    "    X_test_mod = X_mds\n",
    "else:\n",
    "    raise NotImplementedError(f'Model designation: {model} not recognized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SVC from the selected projection and\n",
    "# associated optimized model parameters.\n",
    "svc_best = svm.SVC(C=C, gamma=gamma, kernel='rbf', \n",
    "                   class_weight='balanced', \n",
    "                   probability=True).fit(X_test_mod, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.1; delta = 0.1\n",
    "x_min, x_max = X_test_mod[:,0].min() - h, X_test_mod[:,0].max() + h\n",
    "y_min, y_max = X_test_mod[:,1].min() - h, X_test_mod[:,1].max() + h\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, delta), np.arange(y_min, y_max, delta))\n",
    "Z = svc_best.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "ax.scatter(X_test_mod[idx_stable,0], X_test_mod[idx_stable,1], \n",
    "           s=30, c='green', marker='o', edgecolors='k', alpha=0.5, label='Stable')\n",
    "ax.scatter(X_test_mod[~idx_stable,0], X_test_mod[~idx_stable,1], \n",
    "           s=30, c='red', marker='o', edgecolors='k', alpha=0.5, label='Unstable')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('1st component')\n",
    "ax.set_ylabel('2nd component')\n",
    "# Re-set limits if needed.\n",
    "#ax.set_xlim(right=70)\n",
    "#ax.set_ylim(top=70)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, IPython, sklearn, scipy, matplotlib, pandas, numpy\n",
    "print(\"Notebook createad with:\\\n",
    "      \\nPython {:s}\\nIPython {:s}\\nScikit-learn {:s}\\nPandas {:s}\\nNumpy \\\n",
    "      {:s}\\nScipy {:s}\\nMatplotlib {:s}\"\\\n",
    "      .format(sys.version[:5], IPython.__version__, sklearn.__version__, \n",
    "              pandas.__version__, numpy.__version__, scipy.__version__, \n",
    "              matplotlib.__version__))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
